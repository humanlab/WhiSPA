diff --git a/pretrain/__pycache__/whispa_config.cpython-310.pyc b/pretrain/__pycache__/whispa_config.cpython-310.pyc
index f087291..ccf4b37 100644
Binary files a/pretrain/__pycache__/whispa_config.cpython-310.pyc and b/pretrain/__pycache__/whispa_config.cpython-310.pyc differ
diff --git a/pretrain/__pycache__/whispa_data.cpython-310.pyc b/pretrain/__pycache__/whispa_data.cpython-310.pyc
index 0e49bf3..eb62e29 100644
Binary files a/pretrain/__pycache__/whispa_data.cpython-310.pyc and b/pretrain/__pycache__/whispa_data.cpython-310.pyc differ
diff --git a/pretrain/__pycache__/whispa_model.cpython-310.pyc b/pretrain/__pycache__/whispa_model.cpython-310.pyc
index ffb7aee..47e0ad8 100644
Binary files a/pretrain/__pycache__/whispa_model.cpython-310.pyc and b/pretrain/__pycache__/whispa_model.cpython-310.pyc differ
diff --git a/pretrain/__pycache__/whispa_utils.cpython-310.pyc b/pretrain/__pycache__/whispa_utils.cpython-310.pyc
index 9d771c8..888fabb 100644
Binary files a/pretrain/__pycache__/whispa_utils.cpython-310.pyc and b/pretrain/__pycache__/whispa_utils.cpython-310.pyc differ
diff --git a/pretrain/whispa_data.py b/pretrain/whispa_data.py
index df9aa5d..e38ca94 100644
--- a/pretrain/whispa_data.py
+++ b/pretrain/whispa_data.py
@@ -11,19 +11,18 @@ load_dotenv()
 
 class AudioDataset(torch.utils.data.Dataset):
     
-    def __init__(self, config, processors, use_psych=False, mode='train'):
+    def __init__(self, config, processors, mode='train'):
         self.config = config
         self.hitop_segments_df = pd.read_csv(f'{os.getenv("HITOP_DATA_DIR")}/whispa_dataset.csv')
         self.wtc_segments_df = pd.read_csv(f'{os.getenv("WTC_DATA_DIR")}/whispa_dataset.csv')
         self.processors = processors
-        self.use_psych = use_psych
         self.mode = mode
 
-        if mode == 'train' and self.use_psych:
+        if mode == 'train' and self.config.n_new_dims:
             # Load SBERT Mean and Standard Dimensional Distribution
-            sbert_emb_path = os.path.join(os.getenv('EMBEDDINGS_DIR'), config.sbert_model_id.replace('sentence-transformers/', ''))
+            sbert_emb_path = os.path.join(os.getenv('EMBEDDINGS_DIR'), config.sbert_model_id.replace('jinaai/', ''))
             sbert_mean = np.load(os.path.join(sbert_emb_path, 'mean_emb.npy')).mean()
-            sbert_std = np.load(os.path.join(sbert_emb_path, 'std_emb.npy')).mean() # THIS IS NOT A TYPO
+            sbert_std = np.load(os.path.join(sbert_emb_path, 'std_emb.npy')).mean()
 
             for feat in ['ope', 'agr', 'ext', 'con', 'neu', 'valence', 'arousal', 'ang', 'anx', 'dep']:
                 # Z-Score Normalization for the psychological features
@@ -57,18 +56,13 @@ class AudioDataset(torch.utils.data.Dataset):
                 except:
                     audio_input = audio_input['input_values']
             audio_inputs.append(audio_input)
-            # if isinstance(self.processor, transformers.models.whisper.processing_whisper.WhisperProcessor) \
-            # or isinstance(self.processor, transformers.models.wav2vec2_bert.processing_wav2vec2_bert.Wav2Vec2BertProcessor):
-            #     audio_inputs = audio_inputs['input_features']
-            # elif isinstance(self.processor, transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor):
-            #     audio_inputs = audio_inputs['input_values']
         
         if self.mode == 'train':
             return (
                 audio_inputs[0],
                 df.iloc[i]['message'],
                 audio_inputs[1],
-                torch.from_numpy(df.iloc[i][4:].to_numpy(dtype=np.float32)).unsqueeze(0) if self.use_psych else None
+                torch.from_numpy(df.iloc[i][4:].to_numpy(dtype=np.float32)).unsqueeze(0) if self.config.n_new_dims else None
             )
         elif self.mode == 'inference':
             return (
diff --git a/pretrain/whispa_train.py b/pretrain/whispa_train.py
index a81ae80..1fa6f38 100644
--- a/pretrain/whispa_train.py
+++ b/pretrain/whispa_train.py
@@ -27,7 +27,6 @@ from transformers import (
 import wandb
 from tqdm import tqdm
 from matplotlib import pyplot as plt
-from pprint import pprint
 
 from pretrain.whispa_config import WhiSPAConfig
 from pretrain.whispa_model import WhiSPAModel
@@ -132,11 +131,6 @@ def load_args():
         type=int,
         help="The number of additional dimensions to be added"
     )
-    parser.add_argument(
-        '--use_psych',
-        action='store_true',
-        help='Specify whether to use psychological features during alignment'
-    )
     parser.add_argument(
         '--loss',
         default='DWD',
@@ -145,7 +139,7 @@ def load_args():
             'MOW',
         ],
         type=str,
-        help='Specify the type of loss criteria during training'
+        help='Specify the type of loss criteria during training. `Default value set to DWD`'
     )
     parser.add_argument(
         "--alpha",
@@ -188,15 +182,11 @@ def load_models(config, load_name):
     )
     whispa = WhiSPAModel(config).to(config.device)
 
-    # Load the pre-trained SBERT model/tokenizer
-    sbert_tokenizer = AutoTokenizer.from_pretrained(
-        config.linguistic_teacher_id,
-        cache_dir=os.getenv('CACHE_DIR'),
-        TOKENIZERS_PARALLELISM=False
-    )
+    # Load the pre-trained SBERT model
     sbert = AutoModel.from_pretrained(
         config.linguistic_teacher_id,
-        cache_dir=os.getenv('CACHE_DIR')
+        cache_dir=os.getenv('CACHE_DIR'),
+        trust_remote_code=True
     ).to(config.device)
 
     # Load the pre-trained HuBERT model/processor
@@ -213,18 +203,18 @@ def load_models(config, load_name):
     if config.device == 'cuda':
         if torch.cuda.is_available():
             gpus = list(range(torch.cuda.device_count()))
-            print(f"\nAvailable GPU IDs: {gpus}")
+            logging.info(f"\nAvailable GPU IDs: {gpus}")
             for i in gpus:
-                print(f"\tGPU {i}: {torch.cuda.get_device_name(i)}")
-            print()
+                logging.info(f"\tGPU {i}: {torch.cuda.get_device_name(i)}")
+            logging.info('\n')
             whispa = torch.nn.DataParallel(whispa, device_ids=gpus)
             sbert = torch.nn.DataParallel(sbert, device_ids=gpus)
             hubert = torch.nn.DataParallel(hubert, device_ids=gpus)
         else:
-            print("CUDA is not available. Only CPU will be used.\n")
+            logging.info("CUDA is not available. Only CPU will be used.\n")
 
     if load_name:
-        print('Instantiating WhiSPA with loaded state dict...')
+        logging.info('Instantiating WhiSPA with loaded state dict...')
         state_dict = torch.load(os.path.join(os.getenv('CHECKPOINT_DIR'), load_name, 'best.pth'))
         try:
             whispa.load_state_dict(state_dict)
@@ -232,20 +222,37 @@ def load_models(config, load_name):
             state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
             whispa.load_state_dict(state_dict)
 
-    return whispa, sbert, hubert, whisper_processor, sbert_tokenizer, hubert_processor
+    return whispa, sbert, hubert, whisper_processor, hubert_processor
 
 
-def plot_loss(train_loss, val_loss, save_name):
-    plt.figure(figsize=(10, 6))
-    plt.plot(range(1, len(train_loss) + 1), train_loss, 'b-', label='Training Loss')
-    plt.plot(range(1, len(val_loss) + 1), val_loss, 'r-', label='Validation Loss')
-    plt.title(f'Training vs Validation Loss Curve')
-    plt.xlabel('Epochs')
-    plt.ylabel('Loss')
-    plt.legend()
-    plt.grid(True)
-    os.makedirs(os.path.join(BASE_DIR, 'loss'), exist_ok=True)
-    plt.savefig(os.path.join(BASE_DIR, f'loss/{save_name}.png'), format='png')
+def load_dataset(config, whisper_processor, hubert_processor):
+    dataset = AudioDataset(config, [whisper_processor, hubert_processor],mode='train')
+
+    # Calculate lengths for the train/val split (80:20)
+    total_size = len(dataset)
+    train_size = int(0.8 * total_size)  # 80% for training
+    val_size = total_size - train_size  # 20% for validation
+
+    # Perform the split
+    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
+    logging.info(f'\tTotal dataset size (N): {total_size}')
+    logging.info(f'\tTraining dataset size (N): {train_size}')
+    logging.info(f'\tValidation dataset size (N): {val_size}')
+
+    return train_dataset, val_dataset
+
+
+# def plot_loss(train_loss, val_loss, save_name):
+#     plt.figure(figsize=(10, 6))
+#     plt.plot(range(1, len(train_loss) + 1), train_loss, 'b-', label='Training Loss')
+#     plt.plot(range(1, len(val_loss) + 1), val_loss, 'r-', label='Validation Loss')
+#     plt.title(f'Training vs Validation Loss Curve')
+#     plt.xlabel('Epochs')
+#     plt.ylabel('Loss')
+#     plt.legend()
+#     plt.grid(True)
+#     os.makedirs(os.path.join(BASE_DIR, 'loss'), exist_ok=True)
+#     plt.savefig(os.path.join(BASE_DIR, f'loss/{save_name}.png'), format='png')
 
 
 def train(
@@ -255,11 +262,21 @@ def train(
     sbert,
     hubert,
     whisper_tokenizer,
-    sbert_tokenizer,
     config,
-    use_psych,
     save_name
 ):
+    # Initialize WandB for logging
+    logging.info(f'\nInitializing WandB...')
+    wandb.init(
+        project="WhiSPAA-Training",
+        config=vars(config),
+        name=save_name,
+        group=save_name,
+        job_type='training',
+        save_code=True
+    )
+
+    # Prepare data loaders
     train_loader = torch.utils.data.DataLoader(
         train_dataset,
         batch_size=config.batch_size,
@@ -292,7 +309,7 @@ def train(
     if config.loss == 'DWD':
         loss_func = dwd_loss
     elif config.loss == 'MOW':
-        loss_func = None  # Not yet implemented
+        loss_func = None # Not yet implemented
 
     sbert.eval()
     train_loss = []
@@ -306,15 +323,6 @@ def train(
         epoch_train_loss = 0.0
 
         for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{config.num_epochs} - Training"):
-
-            # SBERT-based tokenization
-            sbert_inputs = sbert_tokenizer(
-                batch['message'],
-                padding=True,
-                truncation=True,
-                return_tensors='pt'
-            ).to(config.device)
-
             # Whisper-based tokenization
             inputs = whisper_tokenizer(
                 batch['message'],
@@ -329,14 +337,13 @@ def train(
 
                 with torch.no_grad():
                     # Get SBERT's MEAN embedding
-                    sbert_embs = sbert(**sbert_inputs).last_hidden_state
-                    sbert_embs = mean_pooling(sbert_embs, sbert_inputs['attention_mask'])
+                    sbert_embs = sbert.encode(batch['message'], task='classification', truncate_dims=config.hidden_size)
 
                     # Get HuBERT's MEAN embedding
                     hubert_embs = hubert(batch['hubert_inputs'].to(config.device)).last_hidden_state.mean(1)
 
                 # Augment with psychological features
-                psych_embs = batch['psych_emb'].to(config.device) if use_psych else None
+                psych_embs = batch['psych_emb'].to(config.device) if config.n_new_dims else None
 
                 # Get WhiSPA's embedding
                 whispa_embs = whispa(
@@ -353,8 +360,11 @@ def train(
                     psych_embs,
                     config.alpha,
                     config.beta,
+                    config.rho,
+                    config.tau,
                 )
                 epoch_train_loss += loss.item()
+                wandb.log({"train_loss": loss.item()})
 
             # Scale the losses and perform backward pass
             scaler.scale(loss).backward()
@@ -372,15 +382,6 @@ def train(
 
         with torch.no_grad():
             for batch in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{config.num_epochs} - Validation"):
-                
-                # SBERT-based tokenization
-                sbert_inputs = sbert_tokenizer(
-                    batch['message'],
-                    padding=True,
-                    truncation=True,
-                    return_tensors='pt'
-                ).to(config.device)
-
                 # Whisper-based tokenization
                 inputs = whisper_tokenizer(
                     batch['message'],
@@ -392,16 +393,14 @@ def train(
                 
                 # Forward pass
                 with torch.amp.autocast(config.device):
-
                     # Get SBERT's MEAN embedding
-                    sbert_embs = sbert(**sbert_inputs).last_hidden_state
-                    sbert_embs = mean_pooling(sbert_embs, sbert_inputs['attention_mask'])
+                    sbert_embs = sbert.encode(batch['message'], task='classification', truncate_dims=config.hidden_size)
 
                     # Get HuBERT's MEAN embedding
                     hubert_embs = hubert(batch['hubert_inputs'].to(config.device)).last_hidden_state.mean(1)
 
                     # Augment with psychological features
-                    psych_embs = batch['psych_emb'].to(config.device) if use_psych else None
+                    psych_embs = batch['psych_emb'].to(config.device) if config.n_new_dims else None
 
                     # Get WhiSPA's embedding
                     whispa_embs = whispa(
@@ -418,8 +417,11 @@ def train(
                         psych_embs,
                         config.alpha,
                         config.beta,
+                        config.rho,
+                        config.tau,
                     )
                     epoch_val_loss += loss.item()
+                    wandb.log({"train_loss": loss.item()})
 
         # Adjust the learning rate based on validation loss
         scheduler.step(epoch_val_loss)
@@ -444,25 +446,35 @@ def train(
 
         epoch_elapsed_time = timedelta(seconds=time.time() - epoch_start_time)
 
-        # Plot and save loss curves
-        plot_loss(train_loss, val_loss, save_name)
-
-        print(f"Epoch {epoch + 1}/{config.num_epochs}")
-        print(f"\tTraining ({config.loss}) Loss: {avg_train_loss:.4f}")
-        print(f"\tValidation ({config.loss}) Loss: {avg_val_loss:.4f}")
-        print(f"\tLearning Rate: {optimizer.param_groups[0]['lr']}")
-        print(f"\tEpoch Elapsed Time: {epoch_elapsed_time}")
+        # Log epoch information to wandb
+        wandb.log({
+            "epoch": epoch,
+            "avg_train_loss": avg_train_loss,
+            "avg_val_loss": avg_val_loss,
+            "learning_rate": optimizer.param_groups[0]['lr'],
+            "epoch_elapsed_time": epoch_elapsed_time
+        })
+
+        # Log epoch information to console
+        logging.info(f"Epoch {epoch + 1}/{config.num_epochs}")
+        logging.info(f"\tTraining ({config.loss}) Loss: {avg_train_loss:.4f}")
+        logging.info(f"\tValidation ({config.loss}) Loss: {avg_val_loss:.4f}")
+        logging.info(f"\tLearning Rate: {optimizer.param_groups[0]['lr']}")
+        logging.info(f"\tEpoch Elapsed Time: {epoch_elapsed_time}")
     
     total_elapsed_time = timedelta(seconds=time.time() - start_time)
-    print(f"\nTotal Elapsed Time: {total_elapsed_time}")
+    logging.info(f"\nTotal Elapsed Time: {total_elapsed_time}")
+    wandb.finish()
         
 
 def main():
     args = load_args()
+    if not args.save_name:
+        args.save_name = f'model_{time.strftime("%Y-%m-%d-%H-%M-%S")}'
 
-    print('Preparing Model Configuration...')
+    logging.info('Preparing Model Configuration...')
     if args.load_name:
-        print('\tInitializing WhiSPA Config from Load File...')
+        logging.info('\tInitializing WhiSPA Config from Load File...')
         config = torch.load(os.path.join(os.getenv('CHECKPOINT_DIR'), args.load_name, 'config.pth'))
         config.shuffle = not args.no_shuffle
         if config.loss != args.loss:
@@ -488,8 +500,6 @@ def main():
         if config.device != args.device:
             config.device = args.device
     else:
-        if args.n_new_dims:
-            args.use_psych = True
         config = WhiSPAConfig(
             whisper_model_id = args.whisper_model_id,
             pooling_mode = args.pooling_mode,
@@ -508,36 +518,27 @@ def main():
             device = args.device
         )
 
-    print(config)
+    logging.info(config)
 
+    # Save Configuration
     if args.save_name:
-        print(f'\nSaving WhiSPA Config...')
+        if os.path.exists(args.save_name):
+            logging.info(f'WARNING: Overwriting existing model directory!')
+            logging.info(f'\t"{args.save_name}" already exists in "{os.getenv("CHECKPOINT_DIR")}"')
+
+        logging.info(f'\nSaving WhiSPA Config...')
         save_dir = os.path.join(os.getenv('CHECKPOINT_DIR'), args.save_name)
         os.makedirs(save_dir, exist_ok=True)
         config_path = os.path.join(save_dir, 'config.pth')
         torch.save(config, config_path)
 
-    print('\nLoading and Initializing Models with Config...')
-    whispa, sbert, hubert, whisper_processor, sbert_tokenizer, hubert_processor = load_models(config, args.load_name)
-
-    print('\nPreprocessing AudioDataset...')
-    dataset = AudioDataset(config, [whisper_processor, hubert_processor], args.use_psych, mode='train')
-
-    # Calculate lengths for the train/val split (80:20)
-    total_size = len(dataset)
-    train_size = int(0.8 * total_size)  # 80% for training
-    val_size = total_size - train_size  # 20% for validation
-    # Perform the split
-    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
-    print(f'\tTotal dataset size (N): {total_size}')
-    print(f'\tTraining dataset size (N): {train_size}')
-    print(f'\tValidation dataset size (N): {val_size}')
+    logging.info('\nLoading and Initializing Models with Config...')
+    whispa, sbert, hubert, whisper_processor, hubert_processor = load_models(config, args.load_name)
 
-    if args.save_name and os.path.exists(args.save_name):
-        print(f'WARNING: Overwriting existing model directory!')
-        print(f'\t"{args.save_name}" already exists in "{os.getenv("CHECKPOINT_DIR")}"')
+    logging.info('\nPreprocessing AudioDataset...')
+    train_dataset, val_dataset = load_dataset(config, whisper_processor, hubert_processor)
 
-    print('\nStarting Training...')
+    logging.info('\nStarting Training...')
     train(
         train_dataset,
         val_dataset,
@@ -545,20 +546,19 @@ def main():
         sbert,
         hubert,
         whisper_processor.tokenizer,
-        sbert_tokenizer,
         config,
-        args.use_psych,
         args.save_name
     )
 
+    # Save Model Checkpoint
     if args.save_name:
-        print(f'\nSaving WhiSPA Model...')
+        logging.info(f'\nSaving WhiSPA Model...')
         save_dir = os.path.join(os.getenv('CHECKPOINT_DIR'), args.save_name)
         best_path = os.path.join(save_dir, 'best.pth')
         last_path = os.path.join(save_dir, 'last.pth')
         torch.save(whispa.state_dict(), last_path)
-        print(f'\tDone.\t`{best_path}`\n')
-        print(f'\tDone.\t`{last_path}`\n')
+        logging.info(f'\tDone.\t`{best_path}`\n')
+        logging.info(f'\tDone.\t`{last_path}`\n')
 
     torch.cuda.empty_cache()
 
diff --git a/pretrain/whispa_utils.py b/pretrain/whispa_utils.py
index eb02db3..5c7c65e 100644
--- a/pretrain/whispa_utils.py
+++ b/pretrain/whispa_utils.py
@@ -61,7 +61,7 @@ def nce_loss(z_a, z_b, tau=0.1, pooling_mode='sum'):
     return losses.sum() if pooling_mode == 'sum' else losses.mean()
 
 
-def dwd_loss(whispa_embs, sbert_embs, hubert_embs, psych_embs, alpha=0.5, beta=0.5):
+def dwd_loss(whispa_embs, sbert_embs, hubert_embs, psych_embs, alpha=0.5, beta=0.5, rho=0.0, tau=0.1):
     """
         Dual-Weighed Distillation Loss
         ------------------------------
@@ -69,9 +69,12 @@ def dwd_loss(whispa_embs, sbert_embs, hubert_embs, psych_embs, alpha=0.5, beta=0
         ------------------------------
     """
     if psych_embs:
-        return alpha * nce_loss(whispa_embs[:-10], sbert_embs) + beta * nce_loss(whispa_embs[:-10], hubert_embs) + nce_loss(whispa_embs[-10:], psych_embs)
+        return alpha * nce_loss(whispa_embs[:-10], sbert_embs, tau) + \
+            beta * nce_loss(whispa_embs[:-10], hubert_embs, tau) + \
+            rho * nce_loss(whispa_embs[-10:], psych_embs, tau)
     else:
-        return alpha * nce_loss(whispa_embs, sbert_embs) + beta * nce_loss(whispa_embs, hubert_embs)
+        return alpha * nce_loss(whispa_embs, sbert_embs, tau) + \
+            beta * nce_loss(whispa_embs, hubert_embs, tau)
 
 
 def mow_loss():
